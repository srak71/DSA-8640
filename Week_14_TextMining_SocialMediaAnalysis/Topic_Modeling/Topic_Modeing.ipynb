{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC MODELING\n",
    "\n",
    "- Discover topics in a text corpus (a collection of documents)\n",
    "\n",
    "- scikit-learn package is very popular in Python: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html\n",
    "- Gensim module is very popular in Python: http://radimrehurek.com/gensim/\n",
    "\n",
    "\n",
    "\n",
    "## Algorithms\n",
    "\n",
    "- Non-negative matrix factorization (NMF)\n",
    "\n",
    "- Latent Dirchilet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Non-negative Matrix Factorization (NMF or NNMF)\n",
    "\n",
    "- is a group of algorithms in multivariate analysis and linear algebra \n",
    "- where a matrix V is factorized into (usually) two matrices W and H, \n",
    "- with the property that all three matrices have **no negative elements**. \n",
    "\n",
    "This non-negativity makes the resulting matrices easier to inspect. \n",
    "\n",
    "Also, in applications such as processing of audio spectrograms or muscular activity, \n",
    "non-negativity is inherent to the data being considered. \n",
    "\n",
    "Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n",
    "\n",
    "Source:\n",
    "https://en.wikipedia.org/wiki/Non-negative_matrix_factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 1. A matrix V is factorized into two matrices W and H\n",
    "![NMF](NMF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. 2. Matrix factorizatio: hidden topics in 1K tweets\n",
    "![MF](MF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Open the JSON file and create a list of 1K tweets text, corpus_contents, for TFIDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "infile = open('tweet_stream_halloween_1000.json')\n",
    "data = json.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_contents = []\n",
    "\n",
    "for t in data:\n",
    "    corpus_contents.append(t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(corpus_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)  Vectorize the corpus with TfidfVectorizer and create a list of unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set a TfidfVecorizer instance object that remove stopwords (stop_words='english') \n",
    "# and ignore terms that appears less than 2% of the documents (min_df=2).\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 2)\n",
    "\n",
    "# Using the TfidVectorizer instance object, \n",
    "# tokenize all the strings in the corpus \n",
    "# and return document-term TF-IDF matrix comprised of vectors for strings\n",
    "doc_term_matrix = vectorizer.fit_transform(corpus_contents)\n",
    "\n",
    "print(doc_term_matrix.shape) # 1000 documents (tweets) and 887 unique words\n",
    "print(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please create a list of unique vocab, we will use this later\n",
    "unique_words = vectorizer.get_feature_names_out() \n",
    "print(len(unique_words)) #, '# of unique words'\n",
    "print(unique_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) NMF Decomposition using document-term matrix with TfidfVectorizer\n",
    "\n",
    "Scikit-learn NMF\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "# Set the desired number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Set a classifier (i.e. clf) that initializes \n",
    "# the NMF decomposition with the assigned number of topics\n",
    "clf = decomposition.NMF(n_components = num_topics)\n",
    "\n",
    "# Using the classifier object, \n",
    "# transform the document-term TF-IDF matrix to fit the NMF model\n",
    "# and return a decomposed matrix with the number of documents and the number of topics.\n",
    "\n",
    "doc_top_matrix = clf.fit_transform(doc_term_matrix)\n",
    "\n",
    "print(doc_top_matrix.shape) # check the shape of the matrix\n",
    "print(doc_top_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **< name of the classifier >.components_** returns a decomposed matrix with the number of topics and the number of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_term_matrix = clf.components_\n",
    "\n",
    "print(top_term_matrix.shape)\n",
    "print(top_term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Now let's try to see the constructed topics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_1 = clf.components_[0]\n",
    "topic_1[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need indices of top key words of each topic!\n",
    "# How to find them? Sorting? We may lose their origial indices...\n",
    "# You can use np.argsort function!\n",
    "\n",
    "num_top_words = 5 # I want to see top 5 words of each topic\n",
    "\n",
    "# get the indices of 5 largest weights (from smaller to larger)\n",
    "np.argsort(topic_1)[-num_top_words:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# But I need top 5 words from top to down! \n",
    "np.argsort(topic_1)[-num_top_words:][::-1] #[::-1] will change its direction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(topic_1[330], topic_1[370], topic_1[321], topic_1[634], topic_1[747])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we can use unique_words that we created in section (2)\n",
    "print(unique_words[330], unique_words[370], unique_words[321], unique_words[634], unique_words[747])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "topic_words = []\n",
    "num_top_words = 5 # I want to see top 5 words of each topic\n",
    "\n",
    "# go over each component/topic\n",
    "for topic in clf.components_:\n",
    "\n",
    "    # get the indices of 5 largest weights (from smaller to larger)\n",
    "    word_idx = np.argsort(topic)[-num_top_words:]\n",
    "    \n",
    "    temp_lst = []\n",
    "    # let's see the words corresponding to the indices\n",
    "    for idx in word_idx[::-1]: # to access the largest weights first, plesae reverse the sequential object using [::-1]\n",
    "        temp_lst.append(unique_words[idx]) # let's append a keywords of the topic to a temp_lst\n",
    "        \n",
    "    topic_words.append(temp_lst) # let's append a list of keyword of the topic to topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import decomposition\n",
    "import numpy as np\n",
    "\n",
    "infile = open('tweet_stream_halloween_1000.json')\n",
    "data = json.load(infile)\n",
    "infile.close()\n",
    "\n",
    "corpus_contents = []\n",
    "\n",
    "for t in data:\n",
    "    corpus_contents.append(t['text'])\n",
    "    \n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 2)\n",
    "doc_term_matrix = vectorizer.fit_transform(corpus_contents)\n",
    "\n",
    "unique_words = vectorizer.get_feature_names_out() \n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "clf = decomposition.NMF(n_components = num_topics)\n",
    "doc_top_matrix = clf.fit_transform(doc_term_matrix)\n",
    "top_term_matrix = clf.components_\n",
    "\n",
    "topic_words = []\n",
    "num_top_words = 5 \n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[-num_top_words:]\n",
    "    temp_lst = []\n",
    "    for idx in word_idx[::-1]: \n",
    "        temp_lst.append(unique_words[idx])\n",
    "    topic_words.append(temp_lst) \n",
    "    \n",
    "pprint(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Practice: Customizing Stopwords for Topic Modeling with 1K Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_additional_stop_word_list = ['rt', 'https']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn import decomposition\n",
    "import numpy as np\n",
    "\n",
    "infile = open('tweet_stream_halloween_1000.json')\n",
    "data = json.load(infile)\n",
    "infile.close()\n",
    "\n",
    "corpus_contents = []\n",
    "\n",
    "for t in data:\n",
    "    corpus_contents.append(t['text'])\n",
    "\n",
    "my_additional_stop_word_list = ['rt', 'https']\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_word_list)\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = my_stop_words, min_df = 2)\n",
    "doc_term_matrix = vectorizer.fit_transform(corpus_contents)\n",
    "\n",
    "unique_words = vectorizer.get_feature_names_out() \n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "clf = decomposition.NMF(n_components = num_topics)\n",
    "doc_top_matrix = clf.fit_transform(doc_term_matrix)\n",
    "top_term_matrix = clf.components_\n",
    "\n",
    "topic_words = []\n",
    "num_top_words = 5 \n",
    "\n",
    "for topic in clf.components_:\n",
    "    word_idx = np.argsort(topic)[-num_top_words:]\n",
    "    temp_lst = []\n",
    "    for idx in word_idx[::-1]: \n",
    "        temp_lst.append(unique_words[idx])\n",
    "    topic_words.append(temp_lst) \n",
    "    \n",
    "pprint(topic_words)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
