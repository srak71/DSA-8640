{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT ANALYTICS / MINING\n",
    "\n",
    "- Unstructured text data is being generated all the time\n",
    "\n",
    "- Text analytics / Text Mining involves techniques and algorithms for analyzing text\n",
    "\n",
    "- Traditional data mining techniques may be used if text is converted to numerical vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Techniques\n",
    "- NLTK: stemming, stopwords, punctuation, top words\n",
    "- WordCloud: visualization\n",
    "- TF-IDF Vectorizer with sklearn\n",
    "- Topic Modeling with gensim\n",
    "- Sentiment analysis with TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fig. Text Mining Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![text_mining_process](text_mining_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Preprocessing with NLTK (Natural Language Toolkit)\n",
    "\n",
    "To properly use NLTK, you need to download various text corpa by running:\n",
    "- import nltk\n",
    "- nltk.download()\n",
    "\n",
    "Otherwise, you may see error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade nltk\n",
    "#!conda install nltk\n",
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()\n",
    "# please download 'All Packages'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Removing Punctuations & Nomalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Hello!! 2019 was great, isn't it? So is 2020!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "p = string.punctuation\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **maketrans(< intabstring >, < outtabstring >)** returns a translation table that maps each character in the intabstring into the character at the same position in the outtab string. \n",
    ">\n",
    "> Then this table is passed to the translate() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_out = len(p)* \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_p = str.maketrans(p, p_out)\n",
    "table_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.translate(table_p).lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root formâ€”generally a written word form. \n",
    "- Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n",
    "- Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "ss = SnowballStemmer(\"english\") \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps.stem('says')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['string', 'bringing', 'maximum', 'roughly','would',\n",
    "         'multiply', 'provision', 'saying', 'saw', 'dogs', 'churches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print('Word: {}\\tLancaster: {}\\tPorter: {}\\tSnowball: {}\\tWordNet: {}'.format(word,ls.stem(word),ps.stem(word),ss.stem(word),wnl.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of words\n",
    "infile = open('frankenstein.txt')\n",
    "words = infile.read().lower().split() #normalization\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***nltk.FreqDist( ):*** A frequency distribution for the outcomes of an experiment. Formally, a frequency distribution can be defined as a function mapping from each sample to the number of times that sample occurred as an outcome. For example, it will produce a frequency distribution that encodes how often each word occurs in a text.\n",
    ">\n",
    "> http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get frequent words\n",
    "freq = nltk.FreqDist(words)\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq['frankenstein']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a plot of top 10 frequent words\n",
    "%matplotlib inline\n",
    "freq.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Most of them are stopwords... We need to remove stopwords...***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(type(stopwords))\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.append('would')\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('frankenstein.txt')\n",
    "words = infile.read().lower().split() #normalization\n",
    "infile.close()\n",
    "\n",
    "words2 = []\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        words2.append(w)\n",
    "\n",
    "print(len(words))\n",
    "print(len(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from words list, remove stopwords\n",
    "words2 = []\n",
    "for w in words:\n",
    "    if w not in stopwords and len(w) > 1:\n",
    "        words2.append(w)\n",
    "\n",
    "print(len(words))\n",
    "print(len(words2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(words)\n",
    "freq2 = nltk.FreqDist(words2)\n",
    "freq.plot(10)\n",
    "freq2.plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Example (frankenstein.txt)\n",
    "\n",
    "## Getting the word frequency after preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "%matplotlib inline\n",
    "\n",
    "#(1) open a dataset (i.e. textfile)\n",
    "infile = open('frankenstein.txt')\n",
    "content = infile.read()\n",
    "infile.close()\n",
    "\n",
    "#(2) nomalization and removing punctuatiion\n",
    "p = string.punctuation\n",
    "table_p = str.maketrans(p, len(p) * \" \")\n",
    "\n",
    "l_content = content.lower() #normalization\n",
    "n_content = l_content.translate(table_p) #removing punctuation\n",
    "\n",
    "#(3) removing stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.append('could')\n",
    "stopwords.append('would')\n",
    "stopwords.append('upon')\n",
    "\n",
    "words = n_content.split()\n",
    "\n",
    "rs_words = []\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        rs_words.append(w)\n",
    "\n",
    "#(4) lemmatizing\n",
    "wnl = WordNetLemmatizer()\n",
    "        \n",
    "le_words = []\n",
    "for w in rs_words:\n",
    "    le_words.append(wnl.lemmatize(w))\n",
    "\n",
    "#(5) getting the word frequency\n",
    "freq = nltk.FreqDist(le_words)\n",
    "freq.plot(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
